{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsparihar/DeepLearningNotes/blob/master/EmotionsCode_JP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D6sp-VL7v1O",
        "outputId": "a42c6a9a-fae6-4729-f105-c1ba209751c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.12.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.22.4)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.27.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.15.2+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.4)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn\n",
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V24NcZb7A3X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rva2qlP_yxLp",
        "outputId": "98c4855c-013a-4965-f4b1-8cf712a14d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "myDriveDir=\"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_KrXIIuyrcL"
      },
      "outputs": [],
      "source": [
        "def setupKaggle():\n",
        "  #if you've already uploaded kaggle.json before, don't do it again\n",
        "  if not(os.path.exists(\"kaggle.json\")):\n",
        "    files.upload() #upload kaggle.json\n",
        "  !pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !ls ~/.kaggle\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "def downloadDataset(dataset_url, myDriveDir):\n",
        "  dataset_name=os.path.join(myDriveDir,\"draft\")\n",
        "  zip_name=dataset_url.split(\"/\")[1]\n",
        "  !kaggle datasets download -d {dataset_url}\n",
        "  if not(os.path.exists(dataset_name)):\n",
        "    !mkdir {dataset_name}\n",
        "  #!unzip -j -q {zip_name}.zip 'emoooooo/*' -d {dataset_name}\n",
        "  !unzip -q {zip_name}.zip -d {dataset_name}\n",
        "  !rm -f {zip_name}.zip\n",
        "\n",
        "  old_name=os.path.join(dataset_name, os.listdir(dataset_name)[0])\n",
        "  new_name=os.path.join(dataset_name,'raw_data')\n",
        "  os.rename(old_name, new_name)\n",
        "  return new_name #Folder path the data is saved in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVSBxImfy_Fw"
      },
      "outputs": [],
      "source": [
        "from mtcnn import MTCNN\n",
        "detector = MTCNN()\n",
        "def findFace(img,detector):\n",
        "    #error occurs at line below when looping through this function\n",
        "    #IndexError: list index out of range\n",
        "    img_size=img.shape\n",
        "    d=detector.detect_faces(img) #figure out how to supress output\n",
        "    if len(d)> 0:\n",
        "      crop_coords,p,keypoints=d[0].values()\n",
        "    else:\n",
        "      p=0; crop_coords=[0,0,img_size[1],img_size[0]]\n",
        "\n",
        "    return crop_coords, p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evkr1lShzF3p"
      },
      "outputs": [],
      "source": [
        "def cropNsave(img,crop_coords,label,dir, faceProb, ext=\"jpeg\"):\n",
        "  # img:Image array\n",
        "  # crop_coords: crop coordinatates gotten from mtcnn\n",
        "  # label: The emotion of the subject in the image. Should be gotten from the folder name it's found in\n",
        "  # dir: the directory to save the cropped data. This will be different to the dir of the uncropped images\n",
        "  # ext: The file type to save the image as\n",
        "  if not(os.path.exists(dir)):\n",
        "      os.mkdir(dir)\n",
        "  x,y,width,height=crop_coords\n",
        "\n",
        "  cropImage=img[y:y+height,x:x+width,:]\n",
        "  cropImage = cv2.resize(cropImage,(160,160))\n",
        "  #image = Image.fromarray(img, 'RGB')\n",
        "  #cropImage=image.crop((x, y, x+width, y+height))\n",
        "\n",
        "  emotFold=os.path.join(dir,label)\n",
        "  badCrop=os.path.join(emotFold,\"poorlyCropped\")\n",
        "\n",
        "  folders=os.listdir(dir)\n",
        "\n",
        "  if label not in folders:\n",
        "    os.mkdir(emotFold)\n",
        "    os.mkdir(badCrop)\n",
        "\n",
        "  if faceProb<0.95: #if mtcnn wasn't confident in identifying the face, the corresponding image is saved in a subfolder of the labeled folder called poorlyCropped\n",
        "    number=str(len(os.listdir(badCrop))+1)+\"_\"+str(faceProb) #length gives the number of images already saved. 'number' is +1 greater than that.\n",
        "    save_dir=badCrop\n",
        "  else:\n",
        "    number=str(len(os.listdir(emotFold))) # no +1 because poorly cropped folder is included in length and thus needs to be subtracted from count\n",
        "    save_dir=emotFold\n",
        "\n",
        "  filename= label + number +\".\"+ ext #name of the image is it's emotion label with a unique ID number, ID numbers are assinged sequentially.\n",
        "  filepath=os.path.join(save_dir,filename)\n",
        "  cv2.imwrite(filepath, cropImage)\n",
        "  #image.save(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VtrcQPESXrQ"
      },
      "outputs": [],
      "source": [
        "def getUncroppedList(datasetPath):\n",
        "  #datasetPath: path of the raw_data folder, which contains all of the label folders.\n",
        "  #outputs: a dictionary with labelnames as keys and a list of filepaths of uncropped images.\n",
        "  path = Path(datasetPath)\n",
        "  parent=path.parent.absolute() #parent of datasetPath. should be the emotions folder\n",
        "\n",
        "  if \"uncroppedList.pickle\" in os.listdir(parent):\n",
        "    savedPath=os.path.join(parent,\"uncroppedList.pickle\")\n",
        "    with open(savedPath, \"rb\") as file:\n",
        "      return pickle.load(file), parent\n",
        "\n",
        "  uncroppedList={}\n",
        "\n",
        "  labels=os.listdir(datasetPath)\n",
        "\n",
        "  if \"train\" in labels: labels.remove(\"train\"), labels.remove(\"test\")\n",
        "  if \"validate\" in labels: labels.remove(\"validate\")\n",
        "\n",
        "  for label in labels:\n",
        "    labelPath=os.path.join(datasetPath,label)\n",
        "    imageNames=os.listdir(labelPath)\n",
        "    uncroppedList.update({label: imageNames})\n",
        "\n",
        "  return uncroppedList, parent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N1TOFELZcC7"
      },
      "outputs": [],
      "source": [
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "#cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "VWMWzp5w6c9X",
        "outputId": "35795b92-b52f-41f1-adec-a6315e1e1035"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bf7f17cd-50f9-4a92-94f6-ad539fe9197a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bf7f17cd-50f9-4a92-94f6-ad539fe9197a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset_url= \"sanidhyak/human-face-emotions\" #\"abdulwasay551/facial-emotion-100100-pictures\"\n",
        "\n",
        "setupKaggle()\n",
        "datasetPath=downloadDataset(dataset_url,myDriveDir) # directory of the emotions folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKUh8txCzMCQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import copy\n",
        "datasetPath=\"/content/drive/MyDrive/draft/raw_data\"\n",
        "uncroppedBefore, parent=getUncroppedList(datasetPath)\n",
        "uncroppedAfter=copy.deepcopy(uncroppedBefore)\n",
        "\n",
        "saveDir=os.path.join(parent,\"cropped\") #save directory for the cropped dataset, which will be in the parent folder of raw_data\n",
        "#Get bounding box for face, cropp image and save the result in a new directory\n",
        "for label in uncroppedBefore:\n",
        "  labelPath=os.path.join(datasetPath,label)\n",
        "  #imageNames=os.listdir(labelPath)\n",
        "  for image_name in uncroppedBefore[label]:\n",
        "    uncroppedAfter[label].remove(image_name)\n",
        "    fileName=os.path.join(labelPath,image_name)\n",
        "    img=cv2.cvtColor(cv2.imread(fileName), cv2.COLOR_BGR2RGB)\n",
        "    crop_coords, faceProb=findFace(img,detector)\n",
        "    cropNsave(img, crop_coords, label, saveDir, faceProb, ext=\"jpeg\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI5uJUYNcIAe"
      },
      "outputs": [],
      "source": [
        "  #RUN THIS IF THE ABOVE FOR LOOP FINISHES EARLY\n",
        "  #save the list of images that still need to be cropped\n",
        "  savePath=os.path.join(parent,\"uncroppedList.pickle\")\n",
        "  with open(savePath, \"wb\") as file:\n",
        "    pickle.dump(uncroppedAfter, file, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGInh7oHNNG1"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL don't run if not necessary\n",
        "#delete the cropped data from a folder\n",
        "def delContents(dir,ext):\n",
        "    img_names=[f for f in os.listdir(dir) if f.endswith(ext)]\n",
        "    for I in img_names:\n",
        "      f=os.path.join(dir, I)\n",
        "      if os.path.isdir(f):\n",
        "        os.rmdir(f)\n",
        "      else: os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX311Afo4XOO"
      },
      "outputs": [],
      "source": [
        "#label=#\"Surprise\" #name of the folder you want to clear\n",
        "#folderpath=os.path.join(myDriveDir,'emotions','cropped',label)\n",
        "setnames=[\"test\",\"train\",\"validate\"]\n",
        "folder=\"/content/drive/MyDrive/draft/raw_data\"\n",
        "setpaths=[os.path.join(folder,f)  for f in setnames]\n",
        "for setp in setpaths:\n",
        "  testcontents=glob.glob(os.path.join(setp,'*'))\n",
        "  for labpath in testcontents:\n",
        "    delContents(labpath,'jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn9vmjeU_1bM"
      },
      "outputs": [],
      "source": [
        "def makeSplitDir(datasetPath, foldername):\n",
        "  #make directory for train test or validate folders\n",
        "  labels=os.listdir(datasetPath)\n",
        "  setdir=os.path.join(datasetPath, foldername)\n",
        "  labelPaths= [os.path.join(setdir,lab) for lab in labels if lab not in [\"train\",\"test\"] ]\n",
        "\n",
        "  if not(os.path.exists(setdir)):\n",
        "    os.mkdir(setdir)\n",
        "    for p in labelPaths:\n",
        "      os.mkdir(p)\n",
        "  return labelPaths, setdir\n",
        "\n",
        "def splitSamples(datasetPath, split):\n",
        "  #makes train and test (and validate) folder in the parent of datasetPath. Populates those folders with labels\n",
        "  #labelPaths =  glob.glob( os.path.join(datasetPath,'*') )\n",
        "  labels=os.listdir(datasetPath)\n",
        "  labelPaths=[os.path.join(datasetPath,lab) for lab in labels if lab not in [\"train\",\"test\" ]]\n",
        "\n",
        "  testLabPaths, testDir = makeSplitDir(datasetPath,'test') #Function defined in this cell\n",
        "  trainLabPaths, trainDir = makeSplitDir(datasetPath, 'train')\n",
        "  setDirs=[trainDir,testDir]\n",
        "\n",
        "  for L, labPath in enumerate(labelPaths):\n",
        "    filepaths=glob.glob(os.path.join(labPath,'*.*'))\n",
        "    filecount=len(filepaths)\n",
        "    shuffled_indices = torch.randperm(filecount)  #np.random.permutation(filecount)\n",
        "    train_size=split[0]*filecount\n",
        "\n",
        "    for i in shuffled_indices[0:train_size]:\n",
        "      shutil.copy(filepaths[i], trainLabPaths)\n",
        "    for i in shuffled_indices[train_size:]:\n",
        "      shutil.copy(filepaths[i], testLabPaths)\n",
        "\n",
        "  return setDirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzBy7wvOvFZe"
      },
      "outputs": [],
      "source": [
        "#refrence1 : https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data up into train and test\n",
        "#1) with background (raw data)\n",
        "rawPath=\"/content/drive/MyDrive/emotion/raw_data\"\n",
        "setDirsRaw = splitSamples(rawPath, split=[0.8,0.2]) #makes a train and test folder in raw_data\n",
        "#2) without background (cropped data)\n",
        "cropPath=\"/content/drive/MyDrive/emotion/cropped\"\n",
        "setDirsCrop = splitSamples(cropPath, split=[0.8,0.2]) #makes a train and test folder in cropped\n",
        "\n"
      ],
      "metadata": {
        "id": "sNDg4nzc6dVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0b81c4a5-a49d-4d26-af01-6618cd3508a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-daed89499d19>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1) with background (raw data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrawPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/emotion/raw_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msetDirsRaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#makes a train and test folder in raw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#2) without background (cropped data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcropPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/emotion/cropped\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d6ba3ee742d4>\u001b[0m in \u001b[0;36msplitSamples\u001b[0;34m(datasetPath, split)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#makes train and test (and validate) folder in the parent of datasetPath. Populates those folders with labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#labelPaths =  glob.glob( os.path.join(datasetPath,'*') )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mlabelPaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVXTkZZRYRZq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "566478c6-7c1a-4f45-d16f-8635ce5991e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4b6e36f98f36>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get data loaders for raw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainDirRaw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestDirRaw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDirRaw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTrainloaderRaw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "#Get data loaders for raw_data\n",
        "trainDirRaw,testDirRaw=os.rawPath\n",
        "\n",
        "dataset = ImageFolder(root=trainDirRaw)\n",
        "TrainloaderRaw=DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "dataset = ImageFolder(root=testDirRaw)\n",
        "TestloaderRaw=DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "#print to for check only.\n",
        "#print(\"Train dataset length:\", len(trainLoader.dataset))\n",
        "#print(\"Validation dataset length:\", len(testLoader.dataset))\n",
        "#print(\"Test dataset length:\", len(valLoader.dataset))\n",
        "\n",
        "#SIDE GOAL\n",
        "#Artificially unbalance an emotion group."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get data loaders for raw_data\n",
        "trainDirCrop,testDirCrop=setDirsCrop\n",
        "\n",
        "dataset = ImageFolder(root=trainDirCrop)\n",
        "TrainloaderCrop=DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "dataset = ImageFolder(root=testDirCrop)\n",
        "TestloaderCrop=DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "#print to for check only.\n",
        "#print(\"Train dataset length:\", len(trainLoader.dataset))\n",
        "#print(\"Validation dataset length:\", len(testLoader.dataset))\n",
        "#print(\"Test dataset length:\", len(valLoader.dataset))"
      ],
      "metadata": {
        "id": "YK1-vtIM8P_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVfbAtXvr71"
      },
      "outputs": [],
      "source": [
        "saveDir=\"/content/drive/MyDrive/draft/cropped\"\n",
        "setDirs = splitSamples(saveDir, split=[0.8,0.2]) #makes a train and test folder in the parent of datasetPath\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXEZpfoC7KoE"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "valDir=\"/content/drive/MyDrive/draft/cropped/validate\"\n",
        "dataset = ImageFolder(root=valDir)\n",
        "loader=DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYfj3XgvJZuq"
      },
      "outputs": [],
      "source": [
        "#IF .ipynb_checkpoints causing errors run this\n",
        "!rm -rf `find -type d -name .ipynb_checkpoints`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuvKA2DZ9Dgk"
      },
      "outputs": [],
      "source": [
        "for batch in loader:\n",
        "  input, labels = batch\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOph6jOcVkxV"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader,model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in loader:\n",
        "      #inputs = inputs.to(device).float()\n",
        "      #labels = labels.to(device).long()\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    return 100*correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLRxiZ11WJJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d1390b56-6988-4810-e280-463390c58190"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-02839a995102>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class facenet()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ],
      "source": [
        "class facenet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N29Qxdhwpk2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "82e73aaa-d2db-4e2f-ef82-284e7b52850d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bc9bae763428>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mnum_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainLoader' is not defined"
          ]
        }
      ],
      "source": [
        "#import facenet model, modify it and retrain it on labeled data (Saxon & Zhenyuyou)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Parameters.\n",
        "num_train = len(trainLoader.dataset)\n",
        "num_test = len(testLoader.dataset)\n",
        "num_channels = 3\n",
        "pic_size = 96\n",
        "num_classes = 7\n",
        "\n",
        "# Training parameters.\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "num_epochs = 10\n",
        "\n",
        "X_train = torch.randn(num_train, num_channels, pic_size, pic_size)\n",
        "X_test = torch.randn(num_test, num_channels, pic_size, pic_size)\n",
        "y_train = torch.randint(0, num_classes, (num_train,))\n",
        "y_test = torch.randint(0, num_classes, (num_test,))\n",
        "\n",
        "# Create DataLoader for training and testing data\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define the FaceNet model\n",
        "model = InceptionResnetV1(classify=True, num_classes=num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "# Training loop\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "traning_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      labels_onehot = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
        "      loss = criterion(outputs, labels_onehot)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      traning_losses += [loss.item()]\n",
        "\n",
        "\n",
        "    train_accuracy.append(evaluate(test_loader, model, traning=True))\n",
        "    test_accuracy.append(evaluate(test_loader, model, traning=True))\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: Train Accuracy = {train_accuracy[-1]:.4f}, Test Accuracy = {test_accuracy[-1]:.4f}')\n",
        "\n",
        "# Plot the training and test accuracy\n",
        "plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig(\"train_test_accuracy.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH9a_2X4rd0F"
      },
      "outputs": [],
      "source": [
        "#import resnet model, modify it and retrain it on labeled data (Nancy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nlisr4FpTX0"
      },
      "outputs": [],
      "source": [
        "#facenet feature creation (Saxon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPb10fzWp1te"
      },
      "outputs": [],
      "source": [
        "#unsupervised clustering on train data (Saxon & Nancy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kmgPse1p_W8"
      },
      "outputs": [],
      "source": [
        "#save clustering results in new folders: 1 folder per cluster (Saxon & Nancy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ANo9qBWpIpE"
      },
      "outputs": [],
      "source": [
        "#retrain moded Facenet model on unsupervised labels\n",
        "#Test on original labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEV9Kh34scyd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}